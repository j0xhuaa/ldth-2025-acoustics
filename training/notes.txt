CHECKPOINT: Trained the model on log mel spectrograms and saved to disk, as a basic starting point.
(Also created a mini streamlit dashboard for viewing the spectrograms, mostly to assist with conceptualising audio data.)

NEXT: Evaluate the model against the test data, and determine performance on unseen samples.
(Train a model on 1D waveform array and compare performance)
(Depending on performance, trial an ensemble model, trained on both, or a third feature)
(Add a creative streamlit component that visually classifies the audio on a dashboard for an operator potentially vibe code that in flask)

Evaluation of model_logmel_cnn_v1.0.h5

Test Accuracy: 0.9655
- 96.55% of predictions were correct on X_test dataset
- Generalizes well on unseen data,

Test Loss: 0.1037
- Cross entropy loss, which quantifies the difference between the predicted probablities and the actual labels
- The loss is reasonable here, as even with a high accuracy a high loss can mean there is some error spread across classes.
- Can be considered as a confidence penalty.

Classification Report
- precision: of all the times the model predicted this class, how many were actually correct
- recall: of all the actual samples of this class, how many did the model correctly identify
- f1-score: Balance between precision and recall
- support: the number of true examples of this class in the rest set
The classification report:
              precision    recall  f1-score   support

           0       0.94      0.94      0.94        16
           1       0.96      0.96      0.96        24
           2       1.00      1.00      1.00        18

    accuracy                           0.97        58
   macro avg       0.97      0.97      0.97        58
weighted avg       0.97      0.97      0.97        58


Evaluating on the Useen data

Unseen Test Accuracy: 0.7778
Unseen Test Loss: 0.8237

Classification Report on Unseen Data
              precision    recall  f1-score   support

           0       0.79      0.62      0.70        24
           1       0.81      0.92      0.86        24
           2       0.73      0.79      0.76        24

    accuracy                           0.78        72
   macro avg       0.78      0.78      0.77        72
weighted avg       0.78      0.78      0.77        72



To do
- Review code elements, step by step
- Evaluate why there is a such a mismatch between test data, and unseen data, what could have gone wrong.
- Consider why evaluation differs
- Create a config for constanst so that data is loaded using consistent shape